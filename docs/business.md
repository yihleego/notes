# Business

本文汇总常见业务系统在高并发、海量数据、分布式场景下的设计要点与落地方案，偏工程实践。

## 闪促 Flash Sale

闪促特点是短时间内大量用户涌入，集中读写有限库存。核心思路：削峰、限流、排队、缓存化读、异步化写，尽量把请求拦在系统上游，避免锁冲到数据库。

### 总体策略

- 活动页静态化 + CDN/边缘缓存，缩短链路。
- 多级限流 + 去重 + 风控，保护核心链路。
- 请求排队 + 预扣库存 + 异步下单，把写压力拉平。
- 数据库条件更新兜底，避免超卖。

### 第零层：静态化与预热（CDN/边缘）

- 活动页、商品详情、规则文案静态化，CDN 缓存。
- 预热库存/活动配置到缓存（Redis/Local Cache），设置短 TTL 和热点保护。
- 预热限流规则、黑名单、验证码策略。

### 第一层：客户端优化

- 点击后按钮置灰、倒计时、禁止重复提交。
- JS 层面限制用户在 x 秒内只允许一次请求（如微信摇一摇），本地去重。
- 轻量人机校验（滑块/验证码）、设备指纹。
- 失败友好提示，避免“无脑重试”。

### 第二层：网关层拦截（Nginx、Spring Cloud Gateway）

怎么防止程序员写 for 循环调用？这类业务都需要登录，用 `uid` 即可。站点层对 `uid + device_id + IP` 进行请求计数与去重，甚至不需要统一存储计数，直接站点内存存储即可（计数不准但最简单，比如 Guava 本地缓存）。

- 一个 `uid` 在 5 秒只允许透过 1 个请求。
- 对 5 秒内的无效请求统一返回“已售罄/稍后再试”，避免重试风暴。
- 令牌桶（Gateway）、时间窗口（Sentinel）等限流策略。
- 反爬与风控（黑名单、动态策略），防止 10w 肉鸡刷流量。

### 第三层：服务层拦截

- 写请求入队：只放行可控数量的请求进入库存扣减与订单创建。
- 漏斗/排队机制：仅放大约库存量的流量，超出直接返回“已售罄”。
- 读请求走缓存，异步更新库存值；热点 Key 使用本地缓存 + Redis 双层。
- “模糊化”库存展示（如火车余票：只显示“有票/无票”或区间）。

### 第四层：数据库层

浏览器拦截了大部分请求，站点层限流和页面缓存，服务层又做了写请求排队与数据缓存，每次透到数据库层的请求都是可控的。数据库通过条件更新/乐观锁兜底，避免超卖。

- `UPDATE stock SET num = num - 1 WHERE id = ? AND num > 0`
- 订单与库存分库，热点字段加索引，避免锁升级和全表扫描。
- 通过库存流水表/审计表跟踪扣减，便于对账与排错。

### 库存处理策略

- **预扣库存**：缓存/数据库锁定库存 -> 创建订单 -> 支付成功后确认扣减。
- **超时释放**：订单超时未支付，延迟任务/消息释放库存。
- **防重扣**：同一用户同一活动使用幂等键或唯一索引保证一次扣减。

## 订单 Order

### 订单模型

- 主订单：`order_id`、`user_id`、金额、支付状态、订单状态、渠道。
- 订单明细：SKU/数量/单价/优惠分摊。
- 状态流水：状态变更与操作人，便于追踪与对账。

### 核心挑战

1. **防超卖**：高并发下库存一致性。
2. **幂等性**：防止重复提交。
3. **超时取消**：未支付订单自动关闭。
4. **状态流转**：严谨的状态机。

### 解决方案

#### 1. 防超卖

- **库存预扣 + 超时释放**：支付成功后确认扣减。
- **数据库乐观锁**：`UPDATE stock SET num = num - 1 WHERE id = 1 AND num > 0`。
- **Redis Lua 脚本**：内存原子扣减，异步同步数据库（极端高性能场景）。

#### 2. 幂等性

- **Token/Idempotency-Key**：进入收银台前申请 Token，提交时校验并删除。
- **数据库唯一索引**：`user_id + sku_id + activity_id` 组合索引防止重复插入。
- **去重表**：记录请求幂等键，便于追溯与补偿。

#### 3. 超时取消

- **Redis ZSet**：Score 存过期时间，轮询消费（量级较小）。
- **RabbitMQ/RocketMQ 延时队列**：消息 TTL + 死信队列（DLX）或延时消息等级。
- **时间轮 (Hashed Wheel Timer)**：Netty/Dubbo 的时间轮算法，内存中高效处理海量定时任务。

#### 4. 订单状态机

定义严格的状态流转图，防止非法操作（如直接从“待支付”变为“已完成”）。

### 状态机示例

- `INIT -> WAIT_PAY -> PAID -> SHIPPED -> DONE`
- `WAIT_PAY -> CANCELED`（超时/主动取消）
- `PAID -> REFUNDING -> REFUNDED`

## 库存 Stock

在电商开发中通常有三种核心策略：下单扣减、支付扣减，以及业内最通用的预扣库存（Hybrid）。

| 方案   | 扣减时机            | 优点                               | 缺点                                   | 适用场景                 |
|------|-----------------|----------------------------------|--------------------------------------|----------------------|
| 下单扣减 | 用户点击“提交订单”立即扣库存 | 用户体验最好，下单成功即代表稳拿商品，不会出现付完钱没货的情况。 | 容易被恶意刷单，导致“库存占位”：有人下单但不付钱，导致想买的人买不到。 | 绝大多数非大促场景。           
| 支付扣减 | 用户扫码支付成功后才扣库存   | 绝对不会出现由于不支付导致的库存锁定，库存利用率最高。      | 超卖风险极大：用户下单时还有货，支付完发现货没了。极度伤害用户体验。   | 内部福利发放、库存极大的普通商品。    |
| 预扣库存 | 下单预留，过期释放       | 兼顾两者优点。下单后锁定库存 $N$ 分钟，支付成功则真正核销。 | 实现复杂度较高，需要处理分布式事务和定时任务。              | 主流电商平台（如淘宝、京东）的通用方案。 |

```sql
create table sku
(
    id           bigint primary key not null,
    name         varchar(100)       not null,
    total_stock  int unsigned       not null default 0 comment '总库存',
    locked_stock int unsigned       not null default 0 comment '锁定库存'
);

-- 下单锁定库存
UPDATE sku
SET locked_stock = locked_stock + ?
WHERE id = ?
  AND total_stock >= locked_stock + ?;

-- 支付成功确认出库
UPDATE sku
SET locked_stock = locked_stock - ?,
    total_stock  = total_stock - ?
WHERE id = ?
  AND locked_stock >= ?;

-- 订单取消释放库存
UPDATE sku
SET locked_stock = locked_stock - ?
WHERE id = ?
  AND locked_stock >= ?;
```

### 预扣减库存成功，生成订单失败/宕机

业务并未实际发生，保证库存一致即可。

预扣减库存前，生成订单号，发送一条MQ异步写入一条预扣减日志绑定对应订单号，通过分析库存扣减量可以判断是否出现库存预扣减成功，但是未生成订单的情况。

## 支付 Payment

### 核心流程

1. **请求支付**：生成预支付订单，返回支付参数给前端。
2. **异步通知 (Callback)**：支付渠道（微信/支付宝）回调服务端。
3. **主动查询 (Poll)**：处理回调失败或丢失，定时任务主动查询支付状态。
4. **对账 (Reconciliation)**：每日下载账单文件，与本地订单核对，处理长短款。

### 关键点

- **验签与校验**：验签、金额/币种/商户号校验、IP 白名单。
- **幂等处理**：回调接口必须幂等，基于 `trade_no + status` 去重。
- **状态一致性**：最终一致性，以支付渠道查询结果为准。
- **金额精度**：严禁使用 `Double/Float`，必须使用 `BigDecimal` 或 `Long`（分）。
- **风控与追溯**：记录请求/回调日志，保留支付流水与状态变更。

## 灰度发布 & 全链路流量染色 Gray Release

### 概念

灰度发布（金丝雀发布）是指在黑与白之间能够平滑过渡的一种发布方式。让一部分用户继续用 A，一部分用户开始用 B，如果 B 没有问题，逐步扩大范围，把所有用户迁移到 B。

### 策略

- **按账号**：白名单用户、内部员工。
- **按流量比例**：随机 1% 流量。
- **按地域/IP**：特定区域用户。

### 实践要点

- **Feature Flag**：配置中心/开关平台动态调整比例。
- **观测与回滚**：监控关键指标（成功率、延迟、错误率），一键回滚。
- **隔离策略**：灰度用户优先落到灰度集群，避免“串味”。

### 全链路染色

为了在微服务架构中实现全链路灰度，需要将灰度标记（Tag）在各个组件间透传。

1. **网关层 (Gateway)**：识别请求（Header/Cookie），打上灰度标记（如 `x-gray-version: v2`）。
2. **RPC 层 (Dubbo/Feign)**：通过 ThreadLocal/Metadata 将标记透传到下游服务。
3. **消息队列 (MQ)**：在消息属性中携带标记，消费者根据标记选择消费逻辑。
4. **数据库 (DB)**：影子库/影子表（压测/隔离场景）。

## 海量数据处理 Massive Data

### 基本原则

- 先缓存，后读写分离，再分库分表。
- 先垂直拆分，后水平拆分。
- 在线链路轻，复杂查询走离线/异步。

### 1. 读写分离

主库写，从库读，解决读多写少场景。注意：**主从延迟**问题（写完立即读可能读不到），解决方案：强制读主或缓存标记。

### 2. 分库分表 (Sharding)

当单表数据量超过千万级，B+ 树深度增加，IO 性能下降。

- **垂直分库**：按业务拆分（订单库、用户库）。
- **水平分表**：按某个字段（Sharding Key）Hash 取模或按时间范围拆分。
    - **问题**：非 Sharding Key 查询困难（需异构索引）、分布式事务、聚合查询复杂。
    - **工具**：ShardingSphere、MyCat。

### 3. 冷热分离

- **热数据**：最近 3 个月订单，存 MySQL/TiDB。
- **冷数据**：历史订单迁移到 HBase/Cassandra/Doris/ES/归档库。

### 4. 分区表与归档

- 按时间/业务维度建分区，减少索引范围。
- 定期归档/拆表，控制在线库体积。

### 5. 深分页优化

`LIMIT 1000000, 10` 会导致全表扫描。

- **优化**：`WHERE id > last_id LIMIT 10`（游标法）。
- **优化**：ES 滚动查询（Scroll/Search After）。

### 6. 大查询与离线计算

- 大报表走离线数仓/宽表，避免拖垮在线库。
- 复杂检索使用 ES/OLAP，在线链路只聚合少量数据。

## 即时通信 Instant Messaging

即时通讯（IM）通常需要满足：

1. 消息可靠送达（不能丢消息）；
2. 顺序性（消息顺序要保持一致）；
3. 连接可维护（支持在线状态、心跳、重连等）；
4. 可扩展性（支持多个客户端和服务端通信）。

### 核心能力

- 投递可靠性：服务端落库或入 MQ 后再 ACK。
- 顺序性：会话维度单调序列号，客户端按序消费。
- 去重：`msg_id` + 幂等消费避免重复投递。
- 离线能力：离线消息存储与拉取。

### 目标

- 服务端可动态扩容。
- 多设备同时登录接收消息（PC、Mobile、Pad、Web）。
- 多设备消息投递可靠。
- 多设备踢下线。
- 支持消息送达、消息已读、消息撤回。
- 支持服务端拉取消息。

### 角色

- 注册中心
- API/Connection 接入层
- Broker/Router
- 逻辑 Server
- 存储（消息库、会话库）

### 拓扑方案

IM 系统有 100 个服务集群，还会继续动态增加。100 个服务之间直接交流需要 4950 个 TCP 连接，负担较大。需要在连接数与转发次数之间做权衡。

| 类型                          | 代表方案                              | 优点             | 缺点          | 适用场景             |
|-----------------------------|-----------------------------------|----------------|-------------|------------------|
| **1. 中心化 Broker 模型**        | RabbitMQ、Kafka、NATS、Redis Pub/Sub | 简单，连接数线性（O(n)） | Broker 成为瓶颈 | 一般消息总线、企业 IM     |
| **2. 层次化中继拓扑**              | Hub/Leaf 模式，或区域代理                 | 可扩展，减少跨区流量     | 设计复杂        | 大规模集群分区部署        |
| **3. Gossip 协议**            | SWIM、Serf、Consul                  | 分布式、无中心        | 延迟高，重复消息多   | 状态同步、心跳传播        |
| **4. Overlay 网络（虚拟 P2P 层）** | libp2p、ZeroMQ、Weave Mesh          | 分布式且自动路由       | 复杂度高        | 高动态节点集群          |
| **5. 层次化 Topic 分发**         | MQTT、NATS JetStream               | 高效的主题隔离        | 需合理设计 Topic | IM 房间、群组消息       |
| **6. 自建 Router 中间层**        | 树形、环形、K-ary 树等                    | 控制转发跳数         | 需维护拓扑       | 定制 IM 或内部 RPC 系统 |

#### 方案 A：消息中间层（强烈推荐）

使用类似 NATS/Kafka/RabbitMQ/Redis Stream 这样的消息中间件。

- 每个服务只需与 Broker 建立 1 个 TCP 连接。
- Broker 负责转发、路由、持久化。
- 连接复杂度从 O(n²) 降到 O(n)。
- 消息延迟通常在 1~3ms 以内（内网部署时）。

改进策略：

- Broker 集群成为瓶颈时，采用多 Broker 分片或区域 Broker + 跨区桥接。
- NATS JetStream 或 Kafka 支持分区与副本机制扩展吞吐。

适合：IM 系统、在线通信、通知系统等。

#### 方案 B：层次化 Router 拓扑

将节点划分为若干“路由层”和“终端层”：

- 例如：100 个节点 -> 10 个 Router 节点，每个 Router 负责 10 个下属节点。
- Router 之间全互联（10 个 Router -> 45 条连接）。
- 普通节点只与本地 Router 保持 1 条连接。

特征：

- 总连接数：Router 互联 + n ≈ 45 + 100 = 145 条连接。
- 消息转发最多两跳（终端 -> Router -> 目标 Router -> 终端）。

优点：

- 延迟可控，结构简单。
- Router 可自动选举或按区域划分。
- 类似 XMPP 中的“组件路由”模式。

适合：自己实现 IM 或 RPC 系统、不想引入第三方中间件。

```
     +-----------+                   +------------+
     | Router #1 |----Router Mesh----| Router #10 |
     +-----------+                   +------------+
        /    |    \
      N1    N2    N3     ... 每个 Router 下管理 10 个服务节点
```

#### 方案 C：Gossip 协议 + 按需路由

使用 Gossip/成员发现协议（如 SWIM、Serf、Consul）建立轻量邻居网络。

- 每个节点只与 k 个邻居通信（k << n）。
- 通过多跳传播消息，最终所有节点都可收到。

优点：

- 无中心、动态扩展性强。
- 抗节点故障、自动重构。
- 网络连接复杂度为 O(n)。

缺点：

- 消息延迟较高（多跳传递）。
- 存在重复消息和冗余带宽。

适合：节点数特别大（>1000）、消息小且高冗余容忍的系统（如状态同步、心跳广播）。

### 消息可靠性与写库保护

如何保证消息发送不丢失，以及大量数据写库不会导致数据库崩溃？

- 写入消息存储或 MQ 后再 ACK 给客户端，避免“已送达但未持久化”。
- 使用 Kafka 将消息异步存库（顺序写 + 零拷贝）；Topic 数量过多会带来随机写开销。
- RocketMQ 提供事务消息与可靠投递，性能略低但功能完整。
- 消费端幂等写库（`msg_id` 唯一索引），失败重试或补偿。

### 按体量设计

- 小型系统：2~4 个节点，高可用即可，采用完全互联（Full Mesh）。
- 中型系统：16~32 个节点，引入消息中间层。
- 大型系统：100+ 节点，层次化 Router 拓扑。
- 超大型系统：1000+ 节点，区域化 Broker/Router + 跨区桥接。

### 数据库选型

- 小型系统：有啥用啥（MySQL/MongoDB）。
- 中大型系统：NoSQL/Cassandra。
- 超大型系统：Cassandra + ClickHouse 冷热分离。

## 权限系统 Permission System

### 设计原则

- **最小权限**：默认拒绝，按需授权。
- **职责分离**：关键操作需要多角色协作。
- **可审计**：权限变更与访问记录可追溯。

### Role-Based Access Control

RBAC 通常应用于组织、部门、群组、角色、岗位等不同授权方式，通常包括 RBAC0、RBAC1、RBAC2 和 RBAC3 四种模型。

- RBAC0：最基础模型，用户/角色/权限均为多对多关系。
- RBAC1：引入角色继承（Hierarchical Role）。
- RBAC2：增加角色约束控制，包含静态/动态责任分离。
- RBAC3：RBAC0 + RBAC1 + RBAC2 的整合。

![access_control_rbac.png](images/access_control_rbac.png)

在大型企业应用中，通常按岗位设置角色（Manager、Employee 等）。用户不会被直接赋予权限，而是通过角色获得相应权限。

#### 组件

- 权限 authority = 资源 resource + 操作 action
    - 功能权限
        - 页面权限/菜单权限
        - 按钮权限
        - 元素权限
    - 数据权限
        - 行级权限
        - 字段权限
- 授权 permission
- 角色 role
- 权限包（拓展）package
- 职位（拓展）position

分配机制：

- 一个员工对应一个职位
- 一个职位对应多个角色
- 一个角色对应多个授权

例如：

- 员工：小明
    - 职位：高级财务专员
        - 角色：开票专员
            - 权限
                - 资源：发票
                - 操作：新建
        - 角色：预算填报员
            - 权限
                - 资源：预算填报
                - 操作：新建

多租户下如何分配角色和权限：

1. 先在租户管理系统预置系统级角色。
2. 开通租户时，将权限包分配给该租户。
3. 租户引用系统角色，随系统角色更新而更新。
4. 租户管理员编辑角色，则复制一份角色并记录系统角色 ID 以便回滚。
5. 预制角色权限不会超出权限包范围，需要取交集。

### Attribute-Based Access Control

ABAC 基于策略进行访问控制，即对 RBAC 的环境策略和行为策略进一步控制。每条策略可使用任意属性定义。

属性通常分为三类：Subject 属性、Resource 属性和系统属性。例如每个员工有“类型”，业务系统有“部门”，则可定义策略：只有正式员工访问对应部门的资源时允许。

![access_control_abac.png](images/access_control_abac.png)

### RBAC 与 ABAC 的区别

RBAC 一般用于粗粒度权限控制，ABAC 用于细粒度控制。通常优先使用 RBAC，再用 ABAC 处理特殊权限需求。RBAC 易理解、易管理；ABAC 灵活、可扩展，但策略复杂度高。

## 分布式 ID Distributed ID

### 选型关注点

- 递增性/趋势递增（是否利于索引）
- 长度与可读性
- 是否依赖中心服务
- 时钟回拨风险

### UUID

- 优点：本地生成，不依赖其他库，几乎不会碰撞。
- 缺点：字符串长，不利于索引；部分算法可能暴露 MAC 地址。

### Snowflake

- 优点：趋势递增，性能好，几乎不会碰撞。
- 缺点：强依赖机器时钟，时钟回拨会导致重复或不可用。

### UUID v7

- 优点：基于时间排序，性能与索引友好性优于传统 UUID。
- 缺点：仍然较长，对存储有一定压力。

### BSON ObjectId

- 优点：使用简单，性能好，几乎不会碰撞。
- 缺点：字符串较长，不利于索引。

### Leaf Segment（号段）

- 优点：趋势递增，数据库压力小，易于扩展。
- 缺点：号段耗尽需要续租，存在一定的时效性。

### 基于 Redis

- 优点：有序递增，性能好。
- 缺点：引入复杂度，需考虑持久化与故障恢复。

### 基于数据库批量获取 ID

- 优点：有序递增。
- 缺点：性能一般（批量获取一段 ID 后非严格有序）。

## 短链 Short Link

### 生成策略

1. 申请短域名。
2. 生成短码：
    - 方案 A：自增 ID + Base62 编码（无冲突、可控）。
    - 方案 B：Hash（MurmurHash）+ 冲突检测。
3. 将短码作为 Key，原 URL 作为 Value 保存到 Redis/DB。
4. Nginx/OpenResty 通过 Lua 脚本读取 URL，301/302 重定向。

Guava:

```java
Hashing.murmur3_32_fixed().hashUnencodedChars(url);
```

### 运营与安全

- 冲突处理：冲突时加盐重试或回退到自增 ID。
- 防刷：单 IP/UID 限流、黑名单。
- 过期策略：短链可设置 TTL，过期归档或回收。

## 任务调度 Task Scheduling

### 常见形态

- 定时任务（Cron）
- 延迟任务（TTL/Delay Queue）
- 事件驱动任务（MQ）

### 服务端集群

- 维护任务定义与调度元数据
- 动态启停任务
- 分布式锁/Leader 选举
- 注册中心或配置服务
- 任务中断与重试
- 任务依赖与互斥
- 服务优雅下线

### 客户端集群

- 分片处理
- 幂等执行与失败重试
- 日志与追踪
- 统一封装与文档
- 降低开发者心智负担

## 分布式事务 Distributed Transaction

### 理论基础

- **CAP**：一致性(C)、可用性(A)、分区容错性(P) 三者不可兼得。
- **BASE**：基本可用(BA)、软状态(S)、最终一致性(E)。

### 常见方案

#### 1. Seata (AT 模式)

- **机制**：基于两阶段提交 (2PC) 的改进。
- **过程**：一阶段本地事务提交 + 生成 Undo Log；二阶段全局提交（删除 Log）或全局回滚（根据 Log 反向补偿）。
- **优点**：代码侵入小，对业务透明。
- **缺点**：性能损耗（全局锁）。

#### 2. TCC (Try-Confirm-Cancel)

- **机制**：业务层面实现 Try(预留)、Confirm(确认)、Cancel(撤销) 三个接口。
- **优点**：性能好，无全局锁。
- **缺点**：代码侵入大，开发成本高。

#### 3. 本地消息表 (Local Message Table)

- **机制**：业务执行 + 插入消息表（本地事务）；定时任务扫描消息表投递 MQ；消费者消费成功后回调删除消息。
- **优点**：实现简单，可靠性高，最终一致性。

#### 4. 事务型消息 (RocketMQ)

- **机制**：发送半消息 -> 执行本地事务 -> 提交/回滚消息 -> 消费者消费。
- **优点**：解耦，RocketMQ 原生支持。

#### 5. Saga

- **机制**：长事务拆分为多个本地短事务，由协调器控制执行序列，出错则执行反向补偿操作。

### 选择建议

- 能最终一致就不要分布式强一致。
- 读多写少优先本地消息表，复杂业务优先 TCC/Saga。

## 排行榜 Leaderboard

### 场景

游戏排名、直播打赏榜、微信步数。

### 方案：Redis ZSet

Redis `Sorted Set` 是实现排行榜的最佳选择。

- `ZADD key score member`：更新分数。
- `ZINCRBY key increment member`：增加分数。
- `ZREVRANGE key 0 9`：获取前 10 名。
- `ZRANK/ZREVRANK`：获取个人排名。

### 进阶问题

- **同分排序**：Redis ZSet 分数相同时按字典序排。若需按“先达到者排前面”，可将时间戳作为小数部分加入分数（分数 = 真实分数 + (1 - 时间戳/10^13)）。
- **百万级用户**：直接操作 ZSet 性能尚可，但在千万/亿级时，需分桶或仅维护 Top N，其余走数据库离线统计。
- **冷热分离**：热榜走 Redis，历史榜单归档至数据库。

## Feed 流 (Timeline)

### 场景

微博、朋友圈、抖音关注流。

### 模式

#### 1. 推模式 (Push / Write Fan-out)

- **写扩散**：发布者发帖时，直接写入所有粉丝的收件箱（Redis List/Timeline Table）。
- **优点**：读取快（O(1)）。
- **缺点**：大 V 发帖写入量巨大（千万粉丝需写千万次），造成写入延迟。

#### 2. 拉模式 (Pull / Read Fan-out)

- **读扩散**：用户读取时，临时去关注的所有人的发件箱拉取并排序聚合。
- **优点**：写入极快。
- **缺点**：读取慢，关注人数多时聚合性能差。

#### 3. 混合模式 (Hybrid)

- **大 V**（粉丝数 > N）：走拉模式（粉丝读时去拉大 V 的内容）。
- **普通用户**：走推模式（直接推送到粉丝收件箱）。
- 结合在线/离线状态优化推送策略。

### 关键点

- 关注关系缓存化，增量拉取。
- 热门内容异步回填，避免写放大。
- 去重与过滤（已读/屏蔽）。

## 用户体系与 SSO User System

### 单点登录 (SSO)

在多系统架构中，用户只需登录一次。

- **CAS (Central Authentication Service)**：经典票据验证流程（TGT、ST）。
- **OAuth2 / OIDC**：授权标准，不仅用于 SSO，也用于第三方授权。

### Session vs JWT

- **Session + Redis**：
    - **优点**：状态服务端可控，易于踢人、注销。
    - **缺点**：依赖 Redis，需要存储与网络开销。
- **JWT (JSON Web Token)**：
    - **优点**：无状态，服务端不存储，扩展性好。
    - **缺点**：一旦签发无法撤销（需配合黑名单/短过期时间 + Refresh Token 解决）。

### 安全要点

- 密码存储使用 `bcrypt/argon2`，禁止明文或可逆加密。
- 重要操作开启 MFA/短信/邮箱校验。
- 登录防爆破：滑块、验证码、IP 限流、设备指纹。

## 微服务治理 Microservice Governance

- 服务注册发现
- 服务负载均衡
- 服务熔断
- 服务降级
- 服务限流
- 服务依赖关系
- 服务通信（Rest、RPC、MQ）
- 服务文档
- 服务安全
- 服务上线、下线流程
- 服务编排
- 配置管理
- 自动化测试
- 兼容性治理
- 资源调度
- 容量规划
- 可观测性（指标/日志/链路追踪）

## 推文点赞 Tweet Like

将推文存到 MySQL/Postgres 等行事务数据库，将点赞存到 ClickHouse 等列数据库。

1. 用户点击点赞按钮，接口发送 Kafka 消息，返回响应结果
2. 处理 Kafka 消息
    - 写 ClickHouse，新增一条`| 用户ID | 推文ID | +1 |`的数据；用户取消点赞，则新增一条`| 用户ID | 推文ID | -1 |`的数据（类似冲销？）
    - 写 Redis，无论是否否命中都直接`INCR`，如果不存在会自动从 1 开始
3. 定时每 N 小时扫描各个 Redis 分片的 Key，将 Redis 中的点赞数加到数据库，然后删除 Key
4. 用户查看某个推文时，点赞数 = DB 点赞数 + Redis 点赞数
5. 不活跃的推文通过定时任务就从 Redis 中移除，活跃的推文会通过下次点赞再次在 Redis 中累计
6. 用户二次进入到页面，通过用户 ID 和推文 ID 查询 ClickHouse 的数据判断是否已经点赞（优化）

要点：

- 接口请求不使用 MQ，而是让客户端实现伪异步，主要考虑数据库崩溃场景：避免 MQ 堆积大量消息；用户点赞后重新进入页面发现未点赞会尝试多次点击，导致出现一位用户出现多次点赞
- 通常点赞数不会要求特别精确，如果需要精确，必须通过 ClickHouse 统计，但是相应的会导致性能下降。
- 不要缓存每个用户的每个推文的点赞，避免缓存爆炸式增长，只对正在活跃的推文点赞进行累计。
- 用户点赞，则新增一条`| 用户ID | 推文ID | +1 |`的数据；用户取消点赞，则新增一条`| 用户ID | 推文ID | -1 |`的数据，类似冲销，而不是去更新原来的数据，因为ClickHouse这类数据库编辑和删除操作性能低下。
- 若因为网络抖动或各种原因，用户点了2次取消点赞，会导致点赞数出现负，此时最好使用`max(0, likes)`防止出现负点赞数。

待优化：

- 非热门推文零星点赞如何减少 Redis Key
- 如何快速扫描 Redis Key 把点赞写入数据库

## 你们IM单实例可以处理多少TCP连接

epoll 本身几乎不限制连接数，只要：

- 内核版本 ≥ 2.6；
- 使用边缘触发（ET）模式；
- 避免阻塞 I/O。

它的瓶颈主要是：

- 用户态代码的处理效率；
- 系统调用频率；
- 内存和网络栈。

在高性能服务器（如 Nginx、Redis、Netty）中，
8核8G的机器稳定处理 10~20 万并发连接是常见的上限，每个月 300-600 块，对于企业来说相当能接受。

如何优化：

- CPU 数量 > CPU 频率
- 禁止在 EventLoop 线程中处理任何业务，例如：数据库查询、Redis操作、读写文件、网络调用
- 尽量使用零拷贝技术，例如：sendfile、transferTo
- 理解 Netty 特性和基本原理，总结最佳实践，实现最优代码，例如：Netty 4 的 Channel 处理是线程安全的，则无需通过锁进行额外处理。