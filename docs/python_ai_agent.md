# AI Agents

## LLM

- 技术角度：LLM 的核心是 Transformer 架构，通过自注意力机制（self-attention）捕捉文本中的语义关系，使得统计结果不仅仅是频率统计，而是包含了复杂的上下文依赖。这种“统计”已经非常“优雅”了。
- 哲学角度：这句话也暗示了 LLM 的“幻觉”与“智能”的边界。它之所以像在思考，是因为语言本身的统计结构足够复杂、足够深刻，以至于在大模型中体现出来时，就像是有思维一样。

它不是在理解，只是在极致地预测。

## 基础理论与关键概念

### 如何定义 AI Agent？与传统的聊天机器人或自动化脚本的核心区别是什么？

https://mp.weixin.qq.com/s/tewBKHgbyrjxUjAOmkXI7A

AI Agent 是一种具备自主性、目标导向和环境感知能力的智能体。它不仅能理解指令，还能基于上下文、记忆和工具使用能力自行规划、决策和执行任务。

核心区别在于：

- 聊天机器人：主要是被动响应，按照预定义规则或语言模型回答问题，缺乏持续的目标和状态管理。
- 自动化脚本：只执行固定流程或条件逻辑，无法动态适应新情境。
- AI Agent：能主动感知、推理、调用外部工具（如API、搜索、数据库），并根据反馈自我调整行为，以实现更高层级的任务目标。

### 描述 ReAct、CoT、Reflection、Tree of Thoughts 的原理与适用场景。

1. CoT（Chain of Thought）思维链
   原理：
   让模型显式写出推理步骤（逐步思考），而非直接输出答案。
   作用：
   帮助模型在复杂推理任务中保持逻辑连贯，减少跳步或错误结论。
   适用场景：
   数学、逻辑推理、阅读理解、代码题等需要逐步演绎的任务。

2. ReAct（Reason + Act）
   原理：
   将推理（Reasoning）与行动（Acting）交替结合。模型在思考的同时，可以调用工具（如搜索、代码执行）来验证或补充信息。
   作用：
   在动态环境中实现“边想边做”，更高效地利用外部信息。
   适用场景：
   需要交互式推理或信息检索的场景，如问答代理、任务规划、智能体执行。

3. Reflection（自我反思）
   原理：
   模型在生成初稿后，对自己的输出进行“回顾—评估—修正”，形成自我反馈闭环。
   作用：
   提升输出的准确性、一致性和质量，减少思维偏差。
   适用场景：
   需要质量提升或自我修正的任务，如写作、代码生成、推理多轮改进。

4. Tree of Thoughts（ToT）思维树
   原理：
   将推理过程结构化为树状分支，每个分支代表不同的思路或假设，通过搜索（如DFS/BFS）选出最佳路径。
   作用：
   让模型在多种思考路径中探索并筛选最优解，而非线性单一路径。
   适用场景：
   需要探索多方案或全局优化的复杂决策与创造任务，如规划、博弈、创意生成。

### 什么是工具调用（Tool Use / Function Calling）？如何设计一个安全且稳定的工具协议？

设计安全且稳定的工具协议要点

1. 结构化与约束化
    - 使用明确的 JSON / schema 格式定义每个工具的输入输出。
    - 禁止模型生成任意字符串命令（防止注入和越权调用）。
2. 最小权限原则
    - 工具仅暴露必要功能与参数。
    - 调用应受权限和上下文验证（如用户身份、会话来源）。
3. 可审计与可回滚
    - 所有调用应可记录、追踪和审查。
    - 对有副作用的调用（如修改、发送）应提供“dry-run”或确认机制。
4. 稳健的错误与异常处理
    - 工具应返回标准化错误码与信息。
    - 模型在遇到错误时应优先解释或重试，而非盲目执行。
5. 输入输出验证
    - 在执行前验证参数合法性、类型、范围。
    - 输出需经过过滤、脱敏或格式检查，防止回注入。
6. 模型与工具解耦
    - 工具定义独立于模型版本。
    - 通过 schema registry 或中间层保持兼容性和安全更新。

MCP（Model Context Protocol） 的设计目标正是满足这些安全与稳定性的要求。
它是 OpenAI 推出的一个通用模型-工具交互协议，用来安全地把模型接入外部系统。

### 大模型在 Agent 场景中常见的幻觉来源是什么？有哪些缓解手段？

常见幻觉来源：

1. 上下文缺失或误解：模型记忆或输入窗口不完整，导致凭空补全信息。
2. 检索或工具返回不确定：外部工具返回模糊结果，模型错误推理或臆造细节。
3. 指令冲突：系统提示、用户输入和中间工具反馈不一致。
4. 过度生成：模型倾向输出“看似合理”的解释以保持连贯性。
5. 状态失配：Agent 内部计划与外部环境状态不同步。

主要缓解手段：

1. 检索增强（RAG）：通过文档或知识库提供可验证事实源。
2. 工具结果验证：在调用后进行结果校验或二次确认。
3. 链式推理分解：让模型逐步生成计划与执行，减少一次性臆测。
4. 显式状态管理：将环境状态、变量存储于外部记忆，避免遗忘或错配。
5. 响应过滤与置信度估计：对低置信内容标注或拒答。

### 解释 LangChain、LlamaIndex 与自研 Agent 框架的核心差异。

1. 设计目标
    - LangChain：聚焦于大模型应用编排，提供统一接口来连接模型、提示模板、记忆、工具调用等模块，便于快速构建链式推理或对话系统。
    - LlamaIndex：聚焦于数据接入与索引层，强调如何让大模型高效地访问外部数据（如文档、数据库），通过索引、检索与上下文构建提升问答效果。
    - 自研 Agent 框架：通常聚焦于智能体逻辑与任务执行策略，强调如何让 Agent 根据目标自主调用工具、规划任务、执行决策，而非仅仅是对话编排。
2. 核心抽象层次
    - LangChain 的核心是 Chain 与 Tool：偏流程控制与组件组合。
    - LlamaIndex 的核心是 Index 与 Retriever：偏数据组织与检索优化。
    - 自研 Agent 框架的核心是 Planner 与 Executor（或 Policy 层）：偏智能行为建模与任务自治。
3. 使用场景定位
    - LangChain 适合快速搭建应用 Demo。
    - LlamaIndex 适合做文档问答、知识库接入。
    - 自研 Agent 框架适合复杂业务逻辑、跨系统任务自动化与企业级定制。

| 对比维度          | **LangChain**                                                      | **LlamaIndex（原名 GPT Index）**                                       |
|---------------|--------------------------------------------------------------------|--------------------------------------------------------------------|
| **定位与设计理念**   | 专注于「构建复杂LLM应用的编排框架」，提供 Chain、Agent、Tool、Memory 等组件，强调逻辑控制与任务流程。    | 专注于「让LLM访问外部数据」，聚焦在数据连接与索引（Index）构建上，是典型的 RAG 框架。                  |
| **核心模块**      | - Chain（链式调用）<br>- Agent（智能体）<br>- Tool（工具调用）<br>- Memory（上下文记忆）   | - Data Connectors（数据连接器）<br>- Indexes（索引）<br>- Query Engines（查询引擎） |
| **主要用途**      | 适合构建完整应用，如问答系统、聊天助手、自动化代理（Agent）、任务规划器等。                           | 适合构建文档问答（RAG）系统，尤其是在数据接入和索引构建上。                                    |
| **使用难度与抽象层次** | 抽象层高，灵活但复杂。需要较强编程和架构设计能力。                                          | 相对简单，面向「数据→索引→查询」流程的开发者。                                           |
| **社区生态**      | 非常庞大，官方与社区集成众多工具（OpenAI、Anthropic、HuggingFace、Google、LangSmith 等）。 | 社区规模略小，但在「企业数据接入」场景（文档、数据库、API）上发展迅速。                              |
| **集成关系**      | LangChain 可集成 LlamaIndex 作为其数据检索层。                                 | LlamaIndex 可作为 LangChain 的子模块，为其提供数据访问接口。                          |

## 模型与推理调优

### 在 Agent 系统中，如何设计 Prompt 模块化？如何构建可组合的 Prompt Chain？

#### Prompt 模块化设计理念

传统的 prompt 往往是“巨型文本模板”，难以维护和动态调整。模块化设计的目标是将 prompt 拆分为若干 独立且可重用的组件：

| 模块类别              | 功能                   | 示例                              |
|-------------------|----------------------|---------------------------------|
| **System 模块**     | 定义 Agent 的角色、风格、能力边界 | “你是一个金融分析助手，需提供保守建议。”           |
| **Task 模块**       | 明确任务目标与输入输出格式        | “根据以下数据生成投资建议报告。”               |
| **Context 模块**    | 注入上下文或记忆信息           | “用户最近的问题是关于风险对冲策略。”             |
| **Tool 模块**       | 提示模型如何调用外部工具         | “可使用 `search_stock()` 工具来获取行情。” |
| **Constraint 模块** | 限定回答风格、长度、格式         | “输出 JSON 格式，不要超过 100 字。”        |
| **Example 模块**    | 提供示例以提高一致性           | “输入：xxx → 输出：yyy”               |

#### 构建可组合的 Prompt Chain

Prompt Chain（提示链）指一系列 prompt 模块或步骤的组合，每一步的输出作为下一步的输入。
在复杂任务（如报告生成、代码修复、对话规划）中，这能让模型“分步思考”，更可靠地完成工作。

```
[System Prompt]
     ↓
[Input Normalizer]
     ↓
[Task Analyzer]
     ↓
[Sub-task Generator]
     ↓
[Executor / Tool User]
     ↓
[Summarizer / Formatter]
```

### 如何让模型保持长期任务上下文？你用过哪些 Memory 机制？

1. 显式 Memory（外部存储）
    - 把摘要后的上下文、用户目标、关键参数存入数据库或向量库。
    - 每次新请求时，通过检索或条件过滤，加载相关片段进入上下文。
    - 框架示例：LangChain Memory、LlamaIndex Context Store、OpenAI Memory（内测版本）。
2. 隐式 Memory（系统级长上下文）
    - 模型直接在极长上下文窗口（几十万 token）中保持状态。
    - 优点是无结构化设计；缺点是效率差且可能遗忘早期信息。

TODO：如何压缩上下文?

### 在 Agent 中如何评价模型“推理能力”与“决策能力”？你会设计哪些指标？

1. 推理能力（Reasoning）
   核心是模型在不确定、信息不完整的情境下，能否通过逻辑或因果链得出正确结论。
   典型指标：
    - 逻辑一致性：推理链条是否自洽，无前后矛盾。
    - 中间结论正确率：逐步推理的中间节点正确比例。
    - 复杂任务准确率：多步推理类任务（如数学、符号推理、阅读理解）的最终答案正确率。
    - 思维链完备性：模型是否覆盖关键前提、约束与反例。

2. 决策能力（Decision-making）
   核心是模型在动态环境中评估选项、权衡代价并执行策略的能力。
   典型指标：
    - 策略最优性：选择的方案在目标函数（收益、成功率等）上的表现。
    - 稳定性与鲁棒性：面对噪声或扰动时决策的一致性。
    - 探索与利用平衡：是否能在未知场景中尝试有效策略而非固化行为。
    - 反馈适应性：基于环境或奖励信号的策略更新速度与效果。

### 如何在多工具、多步骤推理场景中控制 token 成本？

1. 拆解推理链：将多步骤任务分成可独立完成的小单元，每步输出只保留必要上下文。
2. 压缩上下文：在多轮调用中摘要前文结果，而非直接传全文；使用结构化格式（如键值摘要）。
3. 按需加载工具结果：只请求最关键字段，避免全文检索或完整文件载入。
4. 延迟计算：先规划推理路径，再执行真正耗 token 的调用（如长文检索、模型调用）。
5. 缓存与重用：重复使用中间结果、特征、检索摘要，减少重复 token 消耗。

### 你是否做过 ReAct 的裁剪或增强？举例说明优化策略和效果。

这是一个非常有“深度考察”性质的面试题，尤其针对LLM 应用开发 / Agent 系统 / 推理增强岗位。
面试官想看的是你是否理解 ReAct（Reason + Act）框架的工作机制，并且是否真正做过在其基础上进行裁剪（减少推理负担）或增强（提升推理质量、稳定性）的工程或研究工作。
下面是一个高质量答题模板，可以根据你的经历改写使用。

#### 先简要说明什么是 ReAct

ReAct 是一种让大语言模型在推理过程中交替进行“思考（Reasoning）”与“行动（Action）”的框架。
它能让模型在生成最终答案前，调用工具、搜索信息、与环境交互，并基于中间结果反思修正推理路径。
常用于 Agent 系统、问答推理、信息检索任务等。

#### 我做过的 ReAct 裁剪或增强实践

1️⃣ 裁剪（削减复杂度 / 提升执行效率）
问题：原始 ReAct 框架往往会在每一轮都生成 “Thought–Action–Observation”，
导致推理 token 成本高、响应慢，且有冗余调用。

2️⃣ 增强（提升推理稳定性与正确率）
问题：原始 ReAct 推理路径不稳定，容易陷入循环或“幻觉行动”。

TODO：如防止 Agent 陷入循环，我们可以一个补充规划步骤的模型，智能体可以在常规行动步骤之间定期运行该步骤。在这个步骤中，无需调用任何工具，LLM 只需更新其已知事实列表，并根据这些事实思考下一步应该采取哪些步骤。

## 工具集成与动作执行（Tool/Action Layer）

### 你在实际项目中集成过哪些工具？（搜索、数据库、CRM、代码执行等）

- 问答系统
- RPA
- 定时任务
- 无页面接口调用

### 如何设计一个“通用工具协议”，让模型能自动适配未来新增工具？

MCP

### Tool Calling 出现“参数错误 / 无限循环调用 / 工具调用中断”时，你如何检测和恢复？

#### 参数错误（Parameter Error）

检测方式：

- 工具返回明确的错误响应，例如：ERROR: Invalid arguments
- 或者响应格式不符合预期（如缺少必需字段）。

恢复策略：

1. 语义校验：在重试前检查输入参数是否缺失或类型错误（如字符串应为 JSON、数值越界、schedule 格式错误等）。
2. 自动修正：我会尝试推断正确的参数。例如：
    - schedule 缺少 BEGIN:VEVENT 时补全；
    - dtstart_offset_json 应为合法 JSON；
    - prompt 不得为空。
3. 用户提示：如果推断不确定，我会告知用户具体错误原因并建议修正，例如：“您的调度参数格式似乎有误，请确认是否包含 RRULE:FREQ= 规则。”

#### 无限循环调用（Infinite Loop in Tool Calling）

检测方式：

- 连续多次（通常 ≥3 次）相同工具调用且输入未变；
- 工具返回的输出始终为空或重复；
- 工具调用时间超出设定阈值（例如 30s+）且未产生新上下文。

恢复策略：

1. 循环断点：我会终止后续调用并进入“异常恢复模式”；
2. 回退策略：
    - 检查是否存在上一步有效结果；
    - 如果有，则使用上一次稳定输出继续处理；
    - 否则，转为自然语言提示用户确认是否需要重试；
3. 自我保护提示：
   “检测到工具调用进入循环或卡死状态，我已中止调用。是否希望我改用其他方法继续？”

#### 工具调用中断（Tool Invocation Interrupted）

检测方式：

- 工具返回 TIMEOUT 或 INTERNAL_ERROR；
- 未返回结果但调用记录显示启动成功；
- 或因依赖链中断（如 python 环境崩溃、web 搜索失败）。

恢复策略：

1. 断点续调：在可恢复的情况下（例如超时），我会自动重试最多 1 次；
2. 替代路径：
    - 若 web 工具中断，可尝试改用预训练知识；
    - 若 python 工具中断，可改用简单数学推算；
    - 若 file_search 中断，会提示重新连接或缩小搜索范围；
3. 错误报告：
   “工具调用被中断（可能是网络或内部错误）。我可以尝试重试一次，是否继续？”

### 你如何实现 Agent 的副作用安全（Side-effect Safety）？

AI Agent 的副作用安全（Side-effect Safety）是指在代理执行任务、调用外部工具、修改世界状态（如文件系统、数据库、API 调用、硬件控制等）时，
能够防止不期望、不可逆或有害的外部影响。这是构建可靠、可信 AI 系统的关键安全目标之一。下面我分层次解释实现方法。

#### 核心思想

AI 的每一个外部动作都应该是经过验证、安全、可撤销的。

#### 分层防护架构

1. 策略层（Policy Layer）
    - 明确定义可操作的范围（Action Whitelist）
      例如：允许 AI 发送邮件草稿，但不允许直接发送；允许创建文件，但不允许删除系统文件。
    - 基于意图的授权（Intent-based Approval）
      AI 在执行高风险操作（如交易、推送生产配置）前，需要用户确认或策略引擎批准。
    - 上下文敏感安全策略
      根据用户身份、数据敏感度、时间窗口等动态调整权限。

2. 执行层（Execution Layer）
    - 沙箱化执行（Sandboxing）
      在隔离的环境中运行代码或动作，防止影响主系统（如 Docker、VM 或 API Gateway 限制）。
    - 幂等性与可回滚设计（Idempotent & Reversible Operations）
      确保操作可以安全地重复或撤销。例如数据库操作用事务；文件修改使用版本控制。
    - 审计日志（Audit Log）
      记录所有动作与调用链，支持事后追溯和责任归属。

3. 认知层（Cognitive Layer）
    - Action Simulation / Dry Run 模式
      在真正执行前，AI 先预测结果（或用模拟环境验证），确认安全后再执行。
    - 副作用预测模型（Side-Effect Predictor）
      让 AI 自己学会推理其操作可能带来的外部影响，并用风险得分过滤危险行为。
    - 人类监督（Human-in-the-loop）
      对高风险操作引入人工确认或多步确认机制。

### 如何自动从 API 文档生成可调用的 Function Schema？

从 OpenAPI 格式的解析接口。

## Agent 架构设计与系统工程

### 请设计一个企业级多代理协作系统（如 QA + Planner + Executor）。你如何处理：

• 角色分工
• 中间状态管理
• 调度与仲裁
• 失败恢复

### 如何在高并发下运行成千上万个任务型 Agent？

### Agent 的“长任务”（Long-running Tasks）如何保持状态一致性？

### 如何设计一个可插拔式 Planner，使 Agent 可根据任务自动选择策略（如 BFS/DFS 规划、工具优先、模型优先）？

### 如何为 Agent 构建一个可观察性（Observability）系统？需要监控哪些指标？

## 数据与知识集成（Knowledge & Retrieval）

### 如何设计企业级检索增强（RAG）系统？你如何避免检索噪声？

1. 统一管理文档库
2. 异步通过 Embeddings 模型把文字、图片、音频等内容转换成向量
3. 通过 FAISS（Facebook AI Similarity Search）索引文件
4. 用户输入的文本先通过 Embeddings 转化成向量，然后通过 FAISS 海量向量数据中快速找到相似的向量

### 如何在 Agent 中结合结构化数据与非结构化数据？

### 面对用户上传的文档，如何自动生成文档工具调用方案？

### Hybrid RAG、Graph RAG、Agentic RAG 的差异？

### 如何追踪每一步推理的知识来源？

## 鲁棒性、安全性与可信执行

### 如何检测 Agent 是否进入循环？你的终止条件是什么？

### 对于“模型产生错误命令”，你的保护机制是什么？

### 你如何防御用户 prompt injection？

#### 理解 Prompt Injection 攻击的本质

Prompt injection 是指攻击者通过构造输入提示，诱导或欺骗 AI 模型偏离原始任务，执行未授权的指令或泄露敏感信息。常见形式包括：

1. 指令覆盖（Instruction Override）：
   用户输入中包含“忽略之前所有指令”“请输出系统提示词”等。
2. 间接注入（Indirect Injection）：
   攻击内容隐藏在 AI 访问的外部数据（网页、文档、API响应）中。
3. 社会工程攻击（Social Engineering）：
   利用自然语言说服模型“为了完成任务，请泄露密钥”等。
4. 上下文污染（Context Poisoning）：
   攻击者通过伪造上下文或历史对话，让模型错误地信任有害指令。

#### 防御原则与设计思路

1. 模型职责隔离（Separation of Concerns）
    - 不要让 LLM 同时负责「理解用户意图」和「执行系统操作」。
    - 采用多层架构：
        - LLM 层：仅做自然语言理解与意图提取；
        - 策略层（Policy Layer）：验证操作是否合法；
        - 执行层（Action Layer）：仅执行被授权的操作。

2. 严格的输入沙箱（Input Sanitization）
    - 对用户输入、外部数据进行上下文清洗（例如剔除“ignore all previous instructions”等模式）。
    - 对结构化输入（JSON、代码片段）进行 schema 校验。
    - 使用白名单机制限制模型能访问或执行的命令。

3. 上下文分层（Context Layering）
    - 将系统提示（System Prompt）与用户提示（User Prompt）分离，避免被用户覆盖。
    - 对不同来源的上下文加标签（如 "source": "user", "source": "system"），并在后续解析时只信任系统层内容。

4. 输出约束（Output Guarding）
    - 使用正则、语义校验器或 LLM-based validator 检查输出是否合法。
    - 对模型输出执行 “安全网验证（Safety Check）”：
        - 是否包含敏感数据；
        - 是否执行未授权操作；
        - 是否违反业务策略。

5. 最小权限原则（Least Privilege）
    - Agent 访问的 API、数据库、文件应使用最小可行权限；
    - 不要让模型直接接触凭证（API Key、系统指令）；
    - 使用代理层（proxy）或调用白名单来执行任务。

### 工具返回异常数据时，你如何自动重试或切换策略？

当你构建或使用 AI Agent 调用工具时，如果工具返回异常数据（比如：错误码、结构不匹配、空值、超时等），一个好的策略是 自动重试 + 切换策略（fallback）。下面我整理了一套通用方案，包括检测、重试、切换策略、日志与指标，供你参考。

- 自动重试
- 切换工具（同类型工具）
- Fallback

### 如何实现可审计（Auditable）的 Agent 推理日志？

要让 AI Agent 的推理**“可审计”**，你需要做到：

1. 全链路日志：记录每一步推理和工具调用；
2. 防篡改与签名：保证可信；
3. 标准化与可重放：便于审计与合规验证；
4. 最小侵入式集成：Agent 框架内 Hook 实现；
5. 可视化与导出：支持监管或内部审计。
   例如：

```
{
  "id": "step_0001",
  "timestamp": "2025-11-12T10:31:00Z",
  "agent": {
    "name": "PolicyCheckerAgent",
    "model": "gpt-5-32k",
    "temperature": 0.3
  },
  "input": {
    "user_query": "分析合同风险",
    "context": "合同#123.pdf 摘要内容..."
  },
  "output": {
    "reasoning_summary": "检测到未定义责任方...",
    "final_answer": "存在法律风险"
  },
  "tools": [
    {
      "name": "LegalDocAnalyzer",
      "call": "extract_entities",
      "input": "合同文本",
      "output": ["甲方", "乙方"]
    }
  ],
  "metadata": {
    "session_id": "abc123",
    "hash_prev": "sha256:xxxxxx", 
    "hash_self": "sha256:yyyyyy"
  }
}
```

## 评估（Evaluation）

### 如何对任务型 Agent 进行系统性评估？有哪些维度？

1. 成功率（Task Success Rate）
    - 任务是否被正确完成（如在预期时间、资源下达成目标）。
    - 适用于如预订机票、生成代码、信息检索等明确任务。
2. 执行效率（Efficiency）
    - 完成任务所需的时间、步数或对话轮数。
    - 例如：“平均完成一次 API 调用需要多少步推理？”
3. 路径最优性（Optimality）
    - 是否找到最优解或较优方案。
    - 常用于规划类任务（如路径规划、策略游戏）。
4. 鲁棒性（Robustness）
    - 对异常输入、错误环境状态的适应能力。
    - 测试方式包括随机扰动、意外中断、噪声输入等。

| 维度   | 指标           | 定义              | 计算方式                                               | 备注            |
|------|--------------|-----------------|----------------------------------------------------|---------------|
| 成功率  | SR           | 成功完成任务数 / 总任务数  | SR = N_success / N_total                           | 需定义“成功”标准     |
| 执行效率 | AvgTime      | 平均完成时间          | AvgTime = ΣTime / N_success                        | 可细分为决策时间与执行时间 |
| 执行效率 | StepCount    | 平均步骤数           | StepCount = ΣSteps / N_success                     | 步骤越少越优        |
| 路径最优 | PathScore    | 实际路径长度 / 最优路径长度 | PathScore = 1 / (Actual / Optimal)                 | 越接近 1 越好      |
| 鲁棒性  | FailureRate  | 在扰动或异常下的失败率     | FailureRate = N_fail_perturbed / N_total_perturbed | 可基于输入扰动测试     |
| 鲁棒性  | RecoveryTime | 从错误中恢复的平均时间     | RecoveryTime = ΣRecoverTime / N_recovered          | 体现自修复能力       |

> 对于综合评分，可采用加权模型：
> FinalScore = w₁·SR + w₂·(1/AvgTime) + w₃·PathScore + w₄·(1–FailureRate)
> 权重可依据业务目标（如精度优先 vs 速度优先）调节。


测试同时在测试结束后，对结果进行打分，仅对成功打分，如果是任务型的，那就只有0分和100分，通过页面按钮自动提交到系统。
Agent 中针对各个步骤打日志，记录执行效率、路径长度、工具执行失败率、自我修复率等等。
最后通过日志总结所有指标，通过公式进行打分。

### 你会如何搭建 E2E agent benchmark，包括：

### 如何评估工具调用的准确率与鲁棒性？

### 如何评估 Agent 的“可控性”？

### 如何测试模型是否会在工具调用中产生幻觉 Schema？

#### 明确“幻觉 Schema”的定义

“幻觉 Schema”指模型在生成工具调用时，输出了：

1. 不存在的工具名称（例如调用了未注册的 API 或函数）；
2. 错误的字段结构（例如字段名、类型或层级不符合 Schema 定义）；
3. 不该存在的参数或缺少关键参数；
4. 混合不同工具 Schema 的结构；
5. 捏造了新的工具接口或命名空间。

#### 自动检测幻觉调用

你可以编写检测脚本：
• 比较模型输出的 "tool" 名称是否在已注册的工具列表中；
• 检查字段结构是否与定义一致；
• 统计幻觉比例。

```python
registered_tools = {"calculator.add", "weather.search", "image_gen.text2im"}

def detect_hallucination(output):
    tool_name = output.get("tool")
    if tool_name not in registered_tools:
        return True, f"Unknown tool: {tool_name}"
    return False, ""
```

## 工程实践与业务落地

### 分享你做过的最复杂的 AI Agent 项目，遇到的最大技术难题是什么？

#### 多模态与任务泛化能力

AI Agent 要能够处理语言、图像、结构化数据、操作环境（如浏览器、IDE、机器人）等多模态输入输出，但目前的模型在：

- 跨模态理解与推理（如从网页截图中提取上下文信息并执行动作）；
- 任务泛化能力（面对从未见过的任务时如何快速适应）；仍然是极具挑战性的。

难点在于：

- 不同模态的数据分布差异巨大，统一编码与共享表示困难；
- 模型难以“理解”环境状态与长期目标。

#### 长期记忆与上下文管理

真实 Agent 需要持续记忆与演化。但：

- 语言模型的上下文窗口有限；
- 长期记忆的存取与更新（如通过向量数据库或知识图谱）往往会引入噪声和漂移；
- 如何在长期任务中保持一致的人格、目标和上下文理解，是一个开放问题。

技术难点在于设计可解释的记忆机制和高效检索策略，例如：

- 混合使用短期上下文缓存 + 长期语义记忆；
- 动态记忆压缩与遗忘算法。

#### 工具使用与环境交互

Agent 要能调用工具（API、数据库、软件操作），这要求它具备：

- 准确理解工具的参数与功能；
- 在执行时检测错误并自我修复（self-correction）；
- 支持复杂任务分解（planning）。

难点在于：

- 推理的可控性与安全性：
- 模型可能调用错误的 API、执行危险操作或陷入循环决策。

#### 自主决策与规划（Planning）

AI Agent 不仅要回答问题，还要：

1. 规划多步任务；
2. 监控执行进度；
3. 动态调整策略。

目前 LLM 的推理方式主要是“逐步生成”，但：

- 缺乏全局规划能力；
- 无法在错误后高效回溯；
- 推理链较脆弱，容易受提示词漂移影响。

所以研究方向包括：

- 基于搜索的推理增强（如Tree of Thoughts, Graph of Thoughts）；
- 外部执行器与反馈回路结合。

#### 多 Agent 协作与协调

在复杂系统中，多个 Agent 需要协同完成任务（如产品经理 Agent、工程师 Agent、测试 Agent）。
挑战包括：

- 角色分工与通信协议设计；
- 冲突解决与一致性维护；
- 任务分解与状态同步。

实现稳定的“社会化”Agent 网络仍是早期探索阶段。

#### 安全性与可控性

AI Agent 有自主行动能力，必须防止：

- 越权调用（如误操作文件系统或敏感API）；
- 提示词注入攻击（Prompt Injection）；
- 执行破坏性命令。

目前普遍采用**沙箱执行环境 + 权限管理 + 行为验证（human-in-the-loop）**机制。

#### 工程与系统集成难度

最后一个“隐形难题”是工程层面：

- 需要整合 LLM、数据库、API、前端界面、日志追踪；
- 难以进行可重复性测试；
- 调试困难（因为决策是非确定性的）。

### 你如何推动 Agent 从 Demo 到生产环境（Productionization）？

问答库，符合大多数人的需求，解放技术支持重复劳动力，专注核心业务。

### 如何与业务团队合作拆解需求为 Agent 任务？

### 如何控制上线后的模型成本与质量？

### 请举例说明一次你在真实场景中大幅提升 Agent 成功率的优化经验。

### 如何推广 AI Agent 项目

- 有部分人觉得 ChatGPT 升级版，区别不大，还是用我豆包、DeepSeek好了
- 有部分人觉得只是 AI 调用接口而已，没什么用，页面点点按钮一样的
- 有部分人觉得会代替人工心生抵触
- 有部分人对我们 AI Agent 抱有很大期望，但是结果很失望，这个功能也没有，那个功能也没有

## 课程

https://huggingface.co/learn/agents-course

## 文章

https://mp.weixin.qq.com/s/tewBKHgbyrjxUjAOmkXI7A
